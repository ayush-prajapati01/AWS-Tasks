{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "@Author: Ayush Prajapati<br>\n",
    "@Date: 11-09-2024<br>\n",
    "@Last Modified by: Ayush Prajapati<br>\n",
    "@Last Modified time: 11-09-2024<br>\n",
    "@Title: S3 CRUD operations using Boto3 Library<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/ayush/.local/lib/python3.10/site-packages (4.66.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a boto3 client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ayush-boto-crud-bucket\n",
      "ayush-demo-bucket1\n",
      "ayushp-static-website\n"
     ]
    }
   ],
   "source": [
    "response = s3.list_buckets()\n",
    "\n",
    "# Output bucket names\n",
    "for bucket in response['Buckets']:\n",
    "    print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading large file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup multipart transfer configuration \n",
    "config = TransferConfig(\n",
    "    multipart_threshold=1024 * 1024 * 8,  # 8 MB threshold for multipart upload\n",
    "    max_concurrency=10,                    # 10 concurrent threads\n",
    "    multipart_chunksize=1024 * 1024 * 8,   # 8 MB chunk size\n",
    "    use_threads=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_large_file_with_progress(local_file_name, bucket_name, s3_key):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uploads a large file to the S3 bucket\n",
    "    Parameters:\n",
    "        local_file_name: name of local file\n",
    "        bucket_name = name of bucket\n",
    "        s3_key = key value in s3\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get file size for the progress bar\n",
    "    file_size = os.path.getsize(local_file_name)\n",
    "\n",
    "    with tqdm(total=file_size, unit='B', unit_scale=True, desc=local_file_name) as pbar:\n",
    "        def progress_hook(bytes_transferred):\n",
    "            pbar.update(bytes_transferred)\n",
    "\n",
    "        s3.upload_file(local_file_name, bucket_name, s3_key, Config=config, Callback=progress_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/Videos/Immaculate-2024.mkv:   0%|          | 0.00/1.54G [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/Videos/Immaculate-2024.mkv: 100%|██████████| 1.54G/1.54G [13:02<00:00, 1.97MB/s]  \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    bucket_name = 'ayush-demo-bucket1'\n",
    "    file_name = '/home/ayush/Videos/Immaculate-2024.mkv'\n",
    "    s3_key = 'Immaculate-2024.mkv'\n",
    "\n",
    "    upload_large_file_with_progress(file_name, bucket_name, s3_key)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading large file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup multipart transfer configuration \n",
    "config = TransferConfig(\n",
    "    multipart_threshold=1024 * 1024 * 8,  # 8 MB threshold for multipart upload\n",
    "    max_concurrency=10,                    # 10 concurrent threads\n",
    "    multipart_chunksize=1024 * 1024 * 8,   # 8 MB chunk size\n",
    "    use_threads=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_large_file_from_s3(bucket_name, s3_key, file_name):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Downloads a large file from the S3 bucket\n",
    "    Parameters:\n",
    "        bucket_name = name of bucket\n",
    "        s3_key = key value in s3\n",
    "        file_name: name of file to be stored\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the file size for the progress bar\n",
    "        file_size = s3.head_object(Bucket=bucket_name, Key=s3_key)['ContentLength']\n",
    "\n",
    "        # Create a progress bar\n",
    "        with tqdm(total=file_size, unit='B', unit_scale=True, desc=s3_key) as pbar:\n",
    "            with open(file_name, 'wb') as f:\n",
    "                # Define a callback function to update the progress bar\n",
    "                def progress_hook(bytes_transferred):\n",
    "                    pbar.update(bytes_transferred)\n",
    "\n",
    "                # Download the file in chunks\n",
    "                s3.download_fileobj(bucket_name, s3_key, f, Config = config, Callback=progress_hook)\n",
    "\n",
    "        print(f\"File {s3_key} downloaded successfully from {bucket_name} to {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Immaculate-2024.mkv: 100%|██████████| 1.54G/1.54G [01:25<00:00, 18.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Immaculate-2024.mkv downloaded successfully from ayush-demo-bucket1 to /home/ayush/Videos/AWS-Immaculate-2024.mkv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    bucket_name = 'ayush-demo-bucket1'\n",
    "    file_name = '/home/ayush/Videos/AWS-Immaculate-2024.mkv'\n",
    "    s3_key = 'Immaculate-2024.mkv'\n",
    "\n",
    "    download_large_file_from_s3(bucket_name, s3_key, file_name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
